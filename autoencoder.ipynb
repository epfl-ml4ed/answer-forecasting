{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4596 users from data.\n",
      "Performed train/validation/test split:\n",
      "Training set size: 8963\n",
      "Validation set size: 2560\n",
      "Test set size: 1281\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocess_lernnavi_data\n",
    "preprocess_lernnavi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    device=device,\n",
    "    metrics_fn=None,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    # getting the size of the batch just to measure the progress\n",
    "    size = len(train_loader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    metrics = []\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        # train step\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute metrics\n",
    "        metrics.append({\n",
    "            'loss': loss.item(),\n",
    "            # 'accuracy': (pred.argmax(1) == y).sum().item() / len(y),\n",
    "        })\n",
    "        if metrics_fn is not None:\n",
    "            metrics[-1] = dict(**metrics[-1], **metrics_fn(model, pred, y))\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Log training progress\n",
    "        if batch % 100 == 0 and verbose:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"\"\"Training [{current:>5d}/{size:>5d}]\n",
    "    batch loss:     {metrics[-1]['loss']:.3e}\"\"\")\n",
    "    # batch accuracy: {metrics[-1]['accuracy'] * 100:.2f}\"\"\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loop(\n",
    "    model,\n",
    "    criterion,\n",
    "    val_loader,\n",
    "    device=device,\n",
    "    metrics_fn=None,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "    metrics = []\n",
    "    tot_data = 0\n",
    "\n",
    "    for X, y in val_loader:\n",
    "        # val step\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        tot_data += len(y)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        # compute metrics\n",
    "        metrics.append({\n",
    "            'loss': loss.item(),\n",
    "            # 'correct': (pred.argmax(1) == y).sum().item()\n",
    "        })\n",
    "        if metrics_fn is not None:\n",
    "            metrics[-1] = dict(**metrics[-1], **metrics_fn(model, pred, y))\n",
    "\n",
    "    # compute the average loss and accuracy\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    print(f\"\"\"Validation\n",
    "    loss:     {metrics_df['loss'].mean():.3e}\"\"\")\n",
    "    # correct:  {metrics_df['correct'].sum()}\n",
    "    # total:    {tot_data}\n",
    "    # accuracy: {metrics_df['correct'].sum() / tot_data * 100:.2f}\"\"\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    epochs=10,\n",
    "    optimizer_fn=optim.Adam,\n",
    "    optimizer_kwargs={},\n",
    "    scheduler=None,\n",
    "    scheduler_kwargs={},\n",
    "    criterion=F.cross_entropy,\n",
    "    device=device,\n",
    "    metrics_fn=None,\n",
    "    verbose=False\n",
    "):\n",
    "    train_loader, val_loader = data_loader()\n",
    "\n",
    "    optimizer = optimizer_fn(model.parameters(), **optimizer_kwargs)\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler = scheduler(optimizer, **scheduler_kwargs)\n",
    "\n",
    "    train_metrics = []\n",
    "    val_metrics = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\n",
    "            f\"---------- Epoch {epoch+1:{math.ceil(math.log10(epochs+1))}d} ----------\")\n",
    "        epoch_train_metrics = train_loop(model, optimizer, scheduler, criterion, train_loader, device, metrics_fn, verbose=verbose)\n",
    "        epoch_val_metrics = val_loop(model, criterion, val_loader, device, metrics_fn)\n",
    "\n",
    "        train_metrics.append(epoch_train_metrics)\n",
    "        val_metrics.append(epoch_val_metrics)\n",
    "\n",
    "    return {\n",
    "        \"train\": pd.DataFrame(chain.from_iterable(train_metrics)),\n",
    "        \"validation\": pd.DataFrame(chain.from_iterable(val_metrics))\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture, dataset, and utilities definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "\t\"\"\"\n",
    "\t\tMLP Encoder: a simple MLP encoder with dropout and relu activation functions\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tlayers (list): list of hidden layers sizes\n",
    "\t\t\tembedding_dim (int): size of the embedding\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, layers, embedding_dim=32):\n",
    "\t\tsuper(MLPEncoder, self).__init__()\n",
    "\t\tself.seq = nn.Sequential(\n",
    "\t\t\t*chain.from_iterable([[nn.LazyLinear(l), nn.ReLU(), nn.Dropout(0.1)] for l in layers]),\n",
    "\t\t\tnn.LazyLinear(embedding_dim)\n",
    "\t\t)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.seq(x)\n",
    "    \n",
    "    \n",
    "class MLPDecoder(nn.Module):\n",
    "\t\"\"\"\n",
    "\t\tMLP Decoder: a simple MLP decoder with dropout and relu activation functions\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tlayers (list): list of hidden layers sizes\n",
    "\t\t\tinput_size (int): size of the original input\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, layers, input_size):\n",
    "\t\tsuper(MLPDecoder, self).__init__()\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.seq = nn.Sequential(\n",
    "\t\t\t*chain.from_iterable([[nn.LazyLinear(l), nn.ReLU(), nn.Dropout(0.1)] for l in layers[::-1]]),\n",
    "\t\t\tnn.LazyLinear(input_size),\n",
    "\t\t)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.seq(x)\n",
    "\n",
    "def get_encoder_decoder_mlp(layers, embedding_dim=32, input_dim=(28, 28)):\n",
    "\tencoder = MLPEncoder(layers, embedding_dim)\n",
    "\tdecoder = MLPDecoder(layers, input_dim)\n",
    "\treturn encoder, decoder\n",
    "\n",
    "class AE(nn.Module):\n",
    "\ttype_to_model = {\n",
    "\t\t\"mlp\": get_encoder_decoder_mlp\n",
    "\t}\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef create(network_type, **kwargs):\n",
    "\t\tbuilder = AE.type_to_model.get(network_type, None)\n",
    "\n",
    "\t\tif not builder:\n",
    "\t\t\traise ValueError(\"Unknown type of model\")\n",
    "\t\treturn AE(*builder(**kwargs))\n",
    "\n",
    "\tdef __init__(self, encoder, decoder):\n",
    "\t\tsuper(AE, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder(x)\n",
    "\t\tx = self.decoder(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef encode(self, x):\n",
    "\t\treturn self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LernnaviDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        Lernnavi Dataset: student dataset from Lernnavi.\n",
    "        The dataset is based on 2 csv files: users.csv and topics.csv.\n",
    "        The users.csv file contains the features of the students and the topics.csv file contains the students' masteries per topic.\n",
    "        The dataset is loaded from the csv files and the features are stored in the users_features and topics_features attributes.\n",
    "        The topics are preprocessed using a multilingual sentence transformer model to generate the topic embedding.\n",
    "        The returned data is the concatenation of the user features and the topic embeddings scaled by their relative student's mastery level.\n",
    "        \n",
    "        Args:\n",
    "            datapath (str): path to the folder containing the csv files\n",
    "    \"\"\"\n",
    "\n",
    "    _nlp_model = None\n",
    "\n",
    "    @property\n",
    "    def nlp_model(self):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        if LernnaviDataset._nlp_model:\n",
    "            return LernnaviDataset._nlp_model\n",
    "        \n",
    "        LernnaviDataset._nlp_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        return LernnaviDataset._nlp_model\n",
    "\n",
    "\n",
    "    def __init__(self, datapath):\n",
    "        self.datapath = datapath\n",
    "\n",
    "        import os\n",
    "        \n",
    "        # load data\n",
    "        # self.users_features = pd.read_csv(os.path.join(datapath, \"users.csv\"), index_col=0)\n",
    "        # self.topics_features = pd.read_csv(os.path.join(datapath, \"topics.csv\"), index_col=0)\n",
    "        # assert self.users_features.shape[0] == self.topics_features.shape[0]\n",
    "\n",
    "        # # make sure that the data is consistent across the two dataframes\n",
    "        # self.users_features = self.users_features.sort_index()\n",
    "        # self.topics_features = self.topics_features.sort_index()\n",
    "        # assert self.users_features.index.equals(self.topics_features.index)\n",
    "        \n",
    "        self.original_data = pd.read_csv(os.path.join(datapath, \"qna.csv\"))\n",
    "        self.data = (\n",
    "            self.original_data\n",
    "                .drop([\"multiple_responses\", \"question\", \"choices\", \"correct\", \"student_answer\", \"start_time\"], axis=1)\n",
    "                .set_index(\"user_id\")\n",
    "                .fillna(0)\n",
    "        )\n",
    "\n",
    "        # preprocess topic data\n",
    "        self._create_topic_embeddings()\n",
    "\n",
    "    def _create_topic_embeddings(self):\n",
    "        # create embeddings for each topic\n",
    "        # self.topic_embeddings = self.nlp_model.encode(self.topics_features.columns.values)\n",
    "        self.topic_embeddings = self.nlp_model.encode(self.data.columns.values)\n",
    "        \n",
    "        # normalize topic embeddings rows\n",
    "        self.topic_embeddings = self.topic_embeddings / np.linalg.norm(self.topic_embeddings, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # data = np.concatenate((\n",
    "        #     # user features\n",
    "        #     # self.users_features.iloc[idx][[\"mastery_mean\", \"normalized_score_weighted_by_difficulty\", \"normalized_score\"]],\n",
    "            \n",
    "        #     # topic embeddings weighted by user topic mastery level\n",
    "        #     # (self.topics_features.iloc[idx].values[:, np.newaxis] * self.topic_embeddings).ravel()\n",
    "        #     (self.data.iloc[idx].values[:, np.newaxis] * self.topic_embeddings).ravel()\n",
    "        # ))\n",
    "        return torch.tensor((self.data.iloc[idx].values[:, np.newaxis] * self.topic_embeddings).ravel(), dtype=torch.float32), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AutoEncoderDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset wrapper for autoencoder training.\n",
    "        Returns an (X, X) pair from the underlying dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, _ = self.dataset[index]\n",
    "        return x, x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def data_loader(dataset=\"lernnavi\"):\n",
    "    _datasets = (None, None)\n",
    "\n",
    "    match dataset:\n",
    "        case \"mnist\":\n",
    "            _datasets = (\n",
    "                datasets.MNIST(root=\"data\", train=True, download=True, transform=transforms.ToTensor()),\n",
    "                datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor()),\n",
    "            )\n",
    "        case \"lernnavi\":\n",
    "            _datasets = (\n",
    "                LernnaviDataset(\"data/lernnavi/qna/train\"),\n",
    "                LernnaviDataset(\"data/lernnavi/qna/validation\"),\n",
    "            )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        AutoEncoderDataset(_datasets[0]),\n",
    "        batch_size=16,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        AutoEncoderDataset(_datasets[1]),\n",
    "        batch_size=16,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LernnaviDataset(\"data/lernnavi/qna/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12288])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomma\\.conda\\envs\\mlbd_epfl\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 ----------\n",
      "Validation\n",
      "    loss:     2.580e+00\n",
      "---------- Epoch 2 ----------\n",
      "Validation\n",
      "    loss:     6.139e-01\n",
      "---------- Epoch 3 ----------\n",
      "Validation\n",
      "    loss:     4.052e+00\n",
      "---------- Epoch 4 ----------\n",
      "Validation\n",
      "    loss:     2.859e+01\n",
      "---------- Epoch 5 ----------\n",
      "Validation\n",
      "    loss:     4.742e+01\n"
     ]
    }
   ],
   "source": [
    "model = AE.create(\n",
    "    \"mlp\",\n",
    "    layers=[4000, 1000],\n",
    "    embedding_dim=512,\n",
    "    input_dim=12288\n",
    ").to(device)\n",
    "\n",
    "steps_per_epoch = len(data_loader()[0])\n",
    "epochs = 5\n",
    "\n",
    "metrics = train_model(\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    data_loader=data_loader,\n",
    "    criterion=F.mse_loss,\n",
    "    optimizer_fn=optim.Adam,\n",
    "    optimizer_kwargs={\n",
    "        \"lr\": 1e-5\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, \"model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "As the validation mean squared error is so low, further investigation is needed.  \n",
    "We calculate the reconstruction error for each student in terms of norm distance between the original and the prediction. This is indicative of how well the autoencoder is able to reconstruct the original data. However, for future steps we might want to consider combinations of metrics, such as the MSE for the student's features and the distance between each pair (input topic, reconstructed topic) to better assess the quality of the compression and reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379.99548, 34243.2, 1687.5343)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device).eval()\n",
    "\n",
    "losses = []\n",
    "original_norm = []\n",
    "difference_norm = []\n",
    "\n",
    "all_data = [\n",
    "    LernnaviDataset(\"data/lernnavi/qna/train\"),\n",
    "    LernnaviDataset(\"data/lernnavi/qna/validation\"),\n",
    "    LernnaviDataset(\"data/lernnavi/qna/test\")\n",
    "]\n",
    "\n",
    "for ds in all_data:\n",
    "    users = range(len(ds))\n",
    "    for i in np.random.choice(users, size=int(0.25*len(users)), replace=False):\n",
    "        original = ds[i][0]\n",
    "        prediction = model(original.to(device)).detach().cpu()\n",
    "        losses.append(F.mse_loss(original, prediction))\n",
    "        original_norm.append(np.linalg.norm(original))\n",
    "        difference_norm.append(np.linalg.norm(original - prediction))\n",
    "\n",
    "np.mean(losses), np.mean(original_norm), np.mean(difference_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings\n",
    "We can use the trained encoder to generate embeddings for the entire dataset. We can include the validation and test set as well, since we are not training the model anymore and we are just using the embeddings to train the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device).eval()\n",
    "\n",
    "all_data = [\n",
    "    LernnaviDataset(\"data/lernnavi/qna/train\"),\n",
    "    LernnaviDataset(\"data/lernnavi/qna/validation\"),\n",
    "    LernnaviDataset(\"data/lernnavi/qna/test\")\n",
    "]\n",
    "\n",
    "embeddings = []\n",
    "user_ids = []\n",
    "times = []\n",
    "\n",
    "for dataset in all_data:\n",
    "    user_ids.extend(dataset.original_data[\"user_id\"].values)\n",
    "    times.extend(dataset.original_data[\"start_time\"].values)\n",
    "\n",
    "    for X, _ in dataset:\n",
    "        X = X.to(device)\n",
    "        embedding = model.encode(X).detach().cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "embeddings_df = pd.DataFrame({\n",
    "    \"embedding\": embeddings,\n",
    "    \"user_id\": user_ids,\n",
    "    \"start_time\": times\n",
    "})\n",
    "embeddings_df[\"start_time\"] = pd.to_datetime(embeddings_df[\"start_time\"])\n",
    "embeddings_df.to_pickle(\"data/lernnavi/qna/question_embeddings.pkl\")\n",
    "embeddings_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "      <th>user_id</th>\n",
       "      <th>start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.05834822, 0.23875548, 0.12567756, -0.23862...</td>\n",
       "      <td>387604</td>\n",
       "      <td>2021-10-31 18:36:44.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.055224773, 0.22952265, 0.12700102, -0.2168...</td>\n",
       "      <td>387604</td>\n",
       "      <td>2021-11-09 07:57:38.255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           embedding  user_id  \\\n",
       "0  [-0.05834822, 0.23875548, 0.12567756, -0.23862...   387604   \n",
       "1  [-0.055224773, 0.22952265, 0.12700102, -0.2168...   387604   \n",
       "\n",
       "               start_time  \n",
       "0 2021-10-31 18:36:44.534  \n",
       "1 2021-11-09 07:57:38.255  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df = pd.DataFrame({\n",
    "    \"embedding\": embeddings,\n",
    "    \"user_id\": user_ids,\n",
    "    \"start_time\": times\n",
    "})\n",
    "embeddings_df[\"start_time\"] = pd.to_datetime(embeddings_df[\"start_time\"])\n",
    "embeddings_df.to_pickle(\"data/lernnavi/qna/question_embeddings.pkl\")\n",
    "embeddings_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multiple_responses</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>correct</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>start_time</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35299</th>\n",
       "      <td>True</td>\n",
       "      <td>Markiere die Sätze mit der korrekten Kommasetz...</td>\n",
       "      <td>[&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Im „hessischen Landb...</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "      <td>2021-05-21 11:16:29.867</td>\n",
       "      <td>393224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35300</th>\n",
       "      <td>True</td>\n",
       "      <td>Markiere die Sätze mit der korrekten Kommasetz...</td>\n",
       "      <td>[&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Im „hessischen Landb...</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>[False, True, False, True]</td>\n",
       "      <td>2021-05-21 11:16:54.135</td>\n",
       "      <td>393232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       multiple_responses                                           question  \\\n",
       "35299                True  Markiere die Sätze mit der korrekten Kommasetz...   \n",
       "35300                True  Markiere die Sätze mit der korrekten Kommasetz...   \n",
       "\n",
       "                                                 choices  \\\n",
       "35299  [<table><tbody><tr><td><p>Im „hessischen Landb...   \n",
       "35300  [<table><tbody><tr><td><p>Im „hessischen Landb...   \n",
       "\n",
       "                           correct                student_answer  \\\n",
       "35299  [False, False, False, True]  [False, False, False, False]   \n",
       "35300  [False, False, False, True]    [False, True, False, True]   \n",
       "\n",
       "                   start_time  user_id  \n",
       "35299 2021-05-21 11:16:29.867   393224  \n",
       "35300 2021-05-21 11:16:54.135   393232  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna = pd.read_pickle(\"data/lernnavi/qna/MULTIPLE_CHOICE_german.pkl\")\n",
    "qna.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multiple_responses</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>correct</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>start_time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Markiere die Sätze mit der korrekten Kommasetz...</td>\n",
       "      <td>[&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Im „hessischen Landb...</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "      <td>2021-05-21 11:16:29.867</td>\n",
       "      <td>393224</td>\n",
       "      <td>[1.219209, -15.155132, 23.729265, -5.7118816, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Markiere die Sätze mit der korrekten Kommasetz...</td>\n",
       "      <td>[&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Im „hessischen Landb...</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>[False, True, False, True]</td>\n",
       "      <td>2021-05-21 11:16:54.135</td>\n",
       "      <td>393232</td>\n",
       "      <td>[1.2418625, -15.439329, 24.171888, -5.8181686,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   multiple_responses                                           question  \\\n",
       "0                True  Markiere die Sätze mit der korrekten Kommasetz...   \n",
       "1                True  Markiere die Sätze mit der korrekten Kommasetz...   \n",
       "\n",
       "                                             choices  \\\n",
       "0  [<table><tbody><tr><td><p>Im „hessischen Landb...   \n",
       "1  [<table><tbody><tr><td><p>Im „hessischen Landb...   \n",
       "\n",
       "                       correct                student_answer  \\\n",
       "0  [False, False, False, True]  [False, False, False, False]   \n",
       "1  [False, False, False, True]    [False, True, False, True]   \n",
       "\n",
       "               start_time  user_id  \\\n",
       "0 2021-05-21 11:16:29.867   393224   \n",
       "1 2021-05-21 11:16:54.135   393232   \n",
       "\n",
       "                                           embedding  \n",
       "0  [1.219209, -15.155132, 23.729265, -5.7118816, ...  \n",
       "1  [1.2418625, -15.439329, 24.171888, -5.8181686,...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = qna.merge(embeddings_df, on=[\"user_id\", \"start_time\"])\n",
    "merged.to_pickle(\"data/lernnavi/qna/MULTIPLE_CHOICE_german_with_embeddings.pkl\")\n",
    "merged.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
